\section{General Discussion}
% Re-iteration on the motivations and goals
We investigated how recursive processing of number agreement is performed by Neural Language Models (NLMs), treating them as `hypothesis generators' for understanding human natural-language processing. To this end, we contrasted how NLMs process successive and nested constructions, and tested resulting predictions about human performance. 

% Summary of the results on a single dependency in NLMs
\subsection{A Sparse Agreement Mechanism Consistently Emerges in NLMs Across Languages and Grammatical Features}
Using number-agreement tasks with a single subject-verb dependency, we first replicated in Italian previous findings reported for an English NLM, and extended these findings to another grammatical feature, namely, gender. We found that for both number and gender agreement a sparse mechanism emerged in an Italian NLM during training. These findings suggest that the emergence of a sparse agreement mechanism in NLM is a robust phenomenon across languages and grammatical features. 

The results are moreover consistent with the recent finding that a sparse agreement mechanism emerged in an NLM trained on an artificial language having deep nested structures (i.e., with more than two levels of nesting), suggesting that the sparsity property is not due to the rare occurrence of deep nested structures in natural language \citep{lakretz2020recursion}. 

The sparsity and specificity of the agreement mechanism suggests that NLMs develop a separate `module' for at least one specific kind of grammatical processing. This is in contrast with semantic information, that NLMs pack into dense embedding vectors, supporting the computation of graded similarity relations \citep{Mikolov:etal:2013a,Jurafsky:Martin:2020}. It is an open question whether semantic and syntactic information is encoded and processed jointly or separately in the `language network' of the human brain \citep{pallier2011cortical}. It was traditionally assumed that syntactic processing takes place in localized brain regions, such as Broca's area \citep[e.g.,][]{dapretto1999form}. However, several studies cast doubt on this view, providing evidence that semantic and syntactic processing in the language network cannot be dissociated from one another \citep{mollica2018high, siegelman2019attempt, fedorenko2020lack}. There are clearly substantial differences between the human brain and NLMs. However, our findings suggest that separating syntactic from semantic processing is computationally advantageous for addressing the language-modeling task \citep[][for related studies.]{ullman2004contributions, o2006biologically, russin2019reilly} 

% Summary of the results on two dependencies in NLMs
\subsection{Processing of Nested Dependencies in NLMs and Humans}
We next explored agreement processing in recursive structures that comprise a subject-verb dependency nested within another one. We first confirmed the prediction stemming from the sparsity of the NLM agreement mechanism. The network exhibits exceptional difficulty in processing an embedded long-range dependency within a nested construction. Since NLMs lack a recursive procedure to handle multiple dependencies, once the number units are taken up for the encoding of the outermost dependency, the network fails to process an embedded long-range dependency. In contrast, we found that NLMs achieve relatively good performance on processing \textit{short-range} embedded dependencies, as they can rely on short-range number units outside the core sparse mechanism. The cooperation between the long- and short-range mechanisms in NLMs therefore allows the network to support the processing of a large proportion of agreement constructions in natural language, failing substantially only on relatively uncommon constructions, having two or more nested dependencies that are all long-range.\footnote{In an analysis of the incidence of embedded clauses, \citet{karlsson2007constraints} showed that center-embedding construction are relatively uncommon and multiple nested dependencies are practically absent from spoken language, and are rare in written language. One prediction from the interaction between short- and long-range units in the model is thus a significant reduction in occurrence of Long-Nested constructions compared to Short-Nested ones in natural-language corpora.}

Human results were found to have both similarities with the agreement-error patterns of NLMs and several important points of discrepancy:

\subsubsection{Main Similarities}
\begin{itemize}
    \item \textbf{Low error-rates on successive dependencies}: humans and the NLM made a relatively small number of agreement errors on the embedded verb of successive dependencies. For NLMs, this is in accordance with the sequential processing observed in its dynamics (Figure \ref{fig:2by2_dynamics}). The agreement mechanism resets after the first dependency and is thus available to process the second one. For humans, these findings are in accordance with the relatively good performance of humans on right-branching constructions \citep{blaubergs1974short, miller1964free}
    \item \textbf{Subject-congruence effect in nested constructions}: in the case of a plural subject attractor, for all verbs, both humans and NLMs made significantly more errors in incongruent cases, in which the main and embedded subjects had opposite grammatical numbers.
    \item \textbf{Higher error-rate on embedded compared to main verbs of nested dependencies}: a positive interaction between verb position and subject congruence was found for both short- and long-range embedded dependencies, suggesting that embedded verbs are more error prone, confirming \textit{Prediction 1}.

    
\end{itemize}

\subsubsection{Main Differences}
    \begin{itemize}
        \item \textbf{NLM performance is worse than chance level on the embedded verb of Long-Nested}: the major difference between NLM and human performance (Figures \ref{fig:error_rates_plural_subject} \& \ref{fig:error_rates_all_conditions}) lies in the behaviour of the NLM with respect to the embedded verb in Long-Nested. The NLM was worse than chance level, meaning that in most trials the network predicts the grammatical number of the embedded verb based on the number of the main subject, which is encoded and carried through by the agreement mechanism. In contrast, human performance is better than chance level (although only marginally so, $p = 0.028$).
        \item \textbf{Prediction 2 was not confirmed in humans}: a strictly related observation is that the NLM made significantly more errors on the embedded dependency when the dependency was long-range. This was not confirmed in humans, where the interaction between subject-congruence and length of embedded dependency was not significant. Humans as well, however, made more errors in the long-range case.
    \end{itemize}
    

Bearing in mind that the NLM is trained on raw text data, and thus no explicit grammatical knowledge is provided to it, the points of similarity between the error patterns of humans and the NLM are intriguing. In successive constructions, the sequential processing by the agreement mechanism explains the low error rate of the NLM, similarly to that of humans. In nested constructions, the cooperation between short- and long-range mechanisms produces agreement-error patterns that are comparable to those observed in humans, with the exception of performance on the embedded verb in Long-Nested.  Note in particular that NLMs and humans agree in finding embedded agreement harder than the one in the main clause, despite the fact that the latter has to always be longer-range.

However, the points of discrepancy raise doubts about whether the agreement mechanism in NLMs could be similar to the one employed by humans. The NLM must have developed such mechanism as a sophisticated solution for the language-modeling task, allowing it to achieve high performance on structures commonly encountered in the data. On such interpretation, the relatively uncommon Long-Nested  construction unveils the limitation of the network, pointing to a major difference between it and human subjects.

The agreement mechanism of the NLM does not support genuine recursive processing. In Short-Nested, the network processes nested dependencies through the collaboration of two \textit{distinct} mechanisms (i.e., short- and long-range). In contrast, a fully recursive mechanism for handling possibly infinite nested constructions, limited only by finite resources, would presumably exhibit self-similarity when processing a subsequent level of the recursive structure. Human performance, however, is known to be similarly constrained by nested constructions. Already one level of nesting, as in object-extracted relative clauses, is known to be relatively difficult to parse \citep[e.g.,][]{traxler2002processing}, and although humans can process two long-range dependencies that are active simultaneously (for example, ``The fact that the employee who the manager hired stole office supplies worried the executive'' \citep{Gibson:1998}), these constructions are quite demanding and are therefore less common in natural language. All the more so, three levels of nesting, such as doubly center-embedded sentences, are known to be already impossible to process, and are thus virtually non-existent in natural language \citep{karlsson2007constraints}. Consequently, agreement mechanisms that handle only relatively shallow grammatical dependencies might be nonetheless a computational solution relevant to cortical dynamics in some brain regions. If so, the main discrepancy with respect to Long-Nested might turn out to be of a quantitative nature. While NLMs can handle only a single long-range dependency and fail on two, humans can handle two simultaneously active long-range dependencies but would fail on three. Further experiments are required to evaluate such interpretation of the results.


\subsection{Comparison with Psycholinguistic Theories of Agreement Processing}
Several theories have been suggested in the psycholinguistic literature to account for processing difficulties and agreement errors when processing nested structures. We now discuss our results in light of some of these theories, which may provide complementary high-level explanations compared to that suggested by the neural model. It might be worth remarking that NLMs have a key advantage compared to existing psycholinguistic theories, as they do not require to assume a grammar or a parsing algorithm. NLMs learn to represent and process underlying structures in natural language by mere training on the language-modeling task, thus minimizing the number of prior assumptions one needs to make, when basing a processing hypothesis upon them.


\subsubsection{Feature Percolation Theories}
Early psycholinguistic theories suggested that the proximity between an intervening noun and a verb determines the probability of making an agreement error \citep{quirk1972grammar}. This `linear-distance hypothesis' was later rejected by empirical findings showing that error rates across a prepositional phrase (PP) are higher compared to those across a relative clause, although in the former case the subject is closer to the verb and the syntactic complexity of the preamble is smaller \citep{bock1992regulating}. Following these findings, Bock and Cutting suggested the `clause-packaging hypothesis', stating that an attractor within the same clause would generate more interference than one in another structural unit. 

More recent studies suggested the `syntactic-distance hypothesis' \citep{vigliocco1995constructing, vigliocco1999sex, franck2002subject}, according to which agreement errors depend on the syntactic distance between the head noun and attractor along the syntactic tree. According to this view, the grammatical feature of the attractor is assumed to `percolate’ up the syntactic-tree during incremental processing. Such feature percolation can influence the grammatical agreement between the head noun and verb through interference. Feature percolation is assumed to take place incrementally during sentence processing. Therefore, the longer the distance from the attractor to the subject-verb path, the lower the likelihood of interference is. The syntactic-distance hypothesis accounts for reduced error rates across relative-clauses compared to PPs, and for additional evidence for which the clause-packaging hypothesis makes inadequate predictions \citep{franck2002subject}.

However, percolation theories have difficulties to account for agreement errors on embedded dependencies, such as in Short- and Long-Nested. In these constructions, the attractor with respect to the embedded dependency is the main subject, and thus resides higher on the syntactic tree. As \citet{wagers2009agreement} note, percolation in such constructions is thus required to happen downwards, whereas percolation theories traditionally assume upward movement through the tree, which could not explain the observed subject-congruence effects. Moreover, syntactic distance between the main and embedded subjects is much greater than that between the subject and attractor in simple constructions with a prepositional phrase. This predicts lower error rates than those reported for PP constructions. However, our results show higher error rates on the embedded verb compared to previously reported errors on PP constructions in Italian \citep{vigliocco1995constructing}.

\subsubsection{Memory-based theories}
In sentence comprehension, previous findings reported processing facilitation in ungrammatical sentences due to attraction effects \citep[e.g., ][]{pearlmutter1999agreement, wagers2009agreement, lago2015agreement}. Self-paced reading paradigms showed humans to process the words following the verb faster in the presence of a plural attractor. Importantly, this effect was reported only for ungrammatical sentences. To account for this grammatical asymmetry, previous studies suggested that a cue-based memory retrieval process \citep{lewis2005activation} is triggered as a repair mechanism following a violation, which, in contrast, would not be triggered in grammatical sentences having no violation. This cue-based memory retrieval process is error prone, and can thus license a wrong verb form in an ungrammatical sentence, explaining the facilitation observed after the verb in ungrammatical sentences only. 


\citet{lewis2005activation} applied the Adaptive Control of Thought-Rational architecture (ACT-R; \citet{anderson2013architecture}) to sentence processing, and suggested that, during incremental processing, the transient syntactic structure of the sentence is represented across memory `chunks’ in declarative memory. During sentence processing, each new word triggers a memory retrieval, at the end of which the word is integrated into one of the memory chunks. In the case of verbs, at the end of the retrieval process, the verb will be associated with the appropriate subject stored in memory, ideally, having the same grammatical number. During retrieval, a `competition' among memory chunks takes place, and the chunk with the greatest number of features matching the verb is most likely to be retrieved. However, erroneous retrievals can occur, due to  noise and similarity between memory chunks, in which case a verb carrying the wrong number might be accidentally licensed. 

Cue-based retrieval processes were proposed as a repair mechanism, triggered in the case of a violation \citep{wagers2009agreement, lago2015agreement}. For example, for the nested constructions Short- and Long-nested, during the processing of the relative clause, a prediction about the number of the embedded verb is generated. If the embedded verb violates this prediction, as in the case of ungrammatical sentences, a cue-based retrieval is triggered in order to check whether the correct feature was missed. An erroneous retrieval can then license a verb with the wrong number, leading to facilitated reading afterwards. In grammatical sentences, no prediction violation occurs and therefore the repair mechanism will not be triggered, explaining the grammatical asymmetry described above. 

In our study, a violation-detection paradigm was used, and therefore a direct comparison with the results from the described self-paced reading experiments and the ACT-R model is not possible. However, we note that processing times on the embedded and main verbs as predicted by the ACT-R model are consistent with our findings. Simulations of sentence processing in the ACT-R model predict greater processing times on embedded compared to main verbs. This increase in processing time is due in part to an extra retrieval cycle associated with retrieving the relative pronoun and attaching a trace to fill the gap in the relative clause structure \citep{lewis2005activation}. An account in terms of cue-based retrieval as a repair mechanism would thus predict more errors on embedded compared to main verbs in nested constructions. However, since processing times cannot be directly mapped onto agreement errors, novel simulation work would be necessary to generate quantitative agreement-error predictions from the ACT-R model, which is beyond the scope of the current study.

A key difference in the error generation process between the two models is that while in the ACT-R based model errors arise during the retrieval process, which occurs after the presentation of the verb, in the NLM agreement errors are estimated one time step before the verb, and are due to a wrong prediction of the next verb. Agreement errors on ungrammatical sentences in the two models are therefore due to different dynamics - erroneous retrievals vs.~erroneous predictions. A possible integration of these different dynamics is an interesting topic for future work.

\section{Conclusion}
Our study illustrates how investigating emergent mechanisms in neural language models can lead to novel explicit hypotheses about linguistic processing in humans, and even to testable predictions about cortical dynamics. The possibility of achieving a mechanistic understanding of natural language processing in modern NLMs can thus inform research in psycho- and neurolinguistics.

Concerning the specific object of our study, we found that the NLM fails to perform genuinely recursive processing of nested constructions. The network develops grammar-sensitive agreement mechanisms for handling  constructions up to one degree of nesting only. However, the NLM behaviour matches in part various patterns of human agreement error data, showing remarkable similarity across various sentence constructions. Future research should further probe the nature of the similarities and differences between NLMs and humans, establishing to what extent they are only quantitative in nature, or to what extent they point to a human capacity to handle genuine recursion.