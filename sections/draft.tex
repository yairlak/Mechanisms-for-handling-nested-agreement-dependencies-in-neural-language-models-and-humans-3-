% Second,  In the memory-based theory, given a cue, interference arises due to different memory chunks that compete among themselves during retrieval. In NLMs, competition between the different number signals from the short- and long-range mechanisms generates interference during the prediction of the correct verb form. The observed subject-congruence effects are therefore explained with interference dynamics by both models. However, the type of interference in both models is very different. In memory-based theories, interference is similarity-based, assuming content-addressable memory, unlike interference in NLMs. Furthermore, while in the memory-based model interference dynamics are triggered by the verb cues, in NLMs, interference takes place before the occurrence of the verb. 

% Moreover, in some cases, the size of subject-congruence effect was greater than that of an incongruent nested attractor. For example, for the embedded verb of Long-Nested, humans and the NLM made more errors due to incongruent subjects (effect-size=) compared to an intervening nested attractor (effect-size=; interaction). In the NLM, the effect of incongruent subjects in this case arises from interference dynamics. Throughout the processing of the main dependency, the long-range number unit output the value of the encoded grammatical number. In the case of incongruent subjects, this information interferes with information encoded in the short-range units that carry the number of the embedded subject. In contrast, in the case of congruent subjects, since the number of both subjects is identical, the activation of the long-range mechanism support the embedded dependency, also in the case of a nested incongruent attractor.
    
% The agreement mechanism of the NLM was found to process successive dependencies sequentially, achieving high performance on Short- and Long-Successive. In nested constructions, the NLM showed subject-congruence effects on all verbs, making more agreement errors when the main and embedded subject differ by their grammatical number. The NLM successfully handled the main dependency, in both Short- and Long-Nested, achieving significantly below chance-level performance also in the case of incongruent subjects. However, the model showed increased difficulty in processing the embedded dependencies in both Short- and Long-Nested, reaching below chance-level performance on the embedded dependency of Long-Nested in the case of incongruent subjects. These findings are in accordance with the prediction based on the sparsity property of the agreement mechanism discussed above (see also, Prediction 1 and 2 in Section \ref{}). 

% Summary of the results in humans
% Humans 
% Similarly to NLMs, humans showed high performance in both Short- and Long-Successive, with a higher baseline of errors across conditions, possibly due to attention lapses, which are not modelled in NLMs. These results are in accordance with the relatively high performance of humans in processing right-branching constructions. Similarly to NLMs, in nested constructions, humans showed subject-congruence effect across all verbs when the attractor was plural, making more errors in incongruent-subject cases. Furthermore, like NLMs, humans had relatively good performance on the main dependency, also when subjects were incongruent, and made significantly more errors on the embedded compared to the main dependency when subjects were incongruent (Prediction 1). However, although the error rate was higher, humans did not make significantly more errors on the embedded verb when the dependency was long- compared to short-range.

% differences

% similarities 

%Previous inquires into NLMs found that, during training, the network develops a sparse neural mechanism, which specializes in the processing of long-range number agreements. These findings predicted processing difficulties on embedded long-range dependencies in recursively nested constructions, since the sparsity of the agreement mechanism is predicted to exclude the processing of two dependencies that are active at once. We tested this prediction in both NLMs and humans. 

% NLMs develop on the one hand a sparse mechanism for structural feature agreement, and on the other hand, use dense representations to process other types of information, such as semantic. This suggests that the emergence of a structure-sensitive `module' for agreement processing in NLMs is an optimal solution consistently found by the network during training.


% Grammatical information akin to agreement is therefore encoded in a small number of dedicated units. Long-range feature units encode values of grammatical features with pseudo-binary activations, and in the English NLM, structural information regarding agreement was found to be encoded by only a single `syntax' unit whose activity traces the subject-verb dependency. In contrast, 


% The error patterns of NLMs therefore suggested two compelling predictions about human performance. The sparsity of the agreement mechanism and the dramatic failure of the model on the embedded dependency predicted that humans will make more errors on the embedded long-range dependency compared to the main one. The compensation mechanism by the short-range units and the relative good performance of the NLM on Short-Nested predicted that humans will make more errors on a long-range embedded dependency compared to a short-range one. We therefore conducted a behavioral experiment with human subjects, using the same constructions tested with NLMs 

In these studies, participants made more errors when the head noun was singular and the attractor plural. Specifically, in comprehension, \citet{wagers2009agreement} showed that processing facilitation of an embedded dependency in ungrammatical sentences occurred when the attractor (the main subject in this case) was plural.


% \subsection{Number Agreement in Humans}
% Agreement in the psycholinguistic literature as a window into syntactic processing - a way to test theories regarding the evolving representations of syntactic structures in the mind of the subject. 

% Understanding the neural basis of high cognitive functions, such as those involved in language, requires a

Two of these units were shown to encode grammatical number (one for singular the other for plural) and robustly carry it across long-range dependencies. The activity of the third unit was shown to follow the structure of the long-range dependency and to convey this information to the two other `number' units via strong synaptic connections. The sparsity of this 

% Already one level of nesting, such as in object-extracted relative clauses "The boy that the sisters watch smiles", is known to be relatively hard to process for humans \citep[e.g., ][]{traxler2002processing}. In such constructions, an inner subject-verb dependency (`sisters'-`watch') is embedded inside a main one (`boy'-`smiles'), and the two should each agree on grammatical number - singular for the main and plural for the embedded dependency. One aspect of the difficulty of such nested constructions arises from the need to process two subject-verb dependencies that are active at once and that might also carry opposite grammatical numbers, as in the current example. 

Generally, the underlying syntactic representations during incremental parsing, and points of processing difficulties, can be probed in experiments by using number agreement as an index. Agreement-error pattern on various syntactic constructions provide rich empirical data to test models for online syntactic processing. Experiments on subject-verb dependencies have indeed led to the development of various hypotheses about online syntactic representations and processing, such as the `linear distance hypothesis' \citep{}, `clause packing hypothesis' \citep{Bock:Miller:1991}, `hierarchical distance hypothesis' \citep{franck2002subject, franck2006agreement} and the involvement of memory-retrieval processes in sentence processing \citep{lewis2005activation, wagers2009agreement, lago2015agreement}.

In another line of research, following recent advances in Natural Language Processing, number agreement has been also studied in Neural Language Models (NLMs) \citep{Linzen:etal:2016}. NLMs are neural-networks based models that are typically trained to predict the next word given a context on a large natural-language corpus \citep{Elman:1991}. NLMs were shown to develop near-human performance on various linguistic tasks \citep{}, which makes them compelling `hypothesis generators' in the study of the cognitive and neural basis of language processing in humans. Treating NLMs as psycholinguistic subjects, most current research in the field focuses on understanding the behaviour of NLMs with respect to various grammatical phenomena, with some work also showing correlations between internal states of the model and said phenomena. Number agreement is an area where some steps have been taken towards mechanistic understanding. Specifically, in a recent study, we showed that NLMs trained on a large corpus of English developed a number-propagation mechanism for long-range dependencies \citep{lakretz2019emergence}, which was found to comprise an exceptionally small number of units in the network, which were dedicated to carry number across long-range dependencies. Implications of this sparsity of the mechanism on number-agreement processing is the core motivation of the current study.