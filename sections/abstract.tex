\abstract{Recursive processing in sentence comprehension is considered a hallmark of human linguistic abilities. However, its underlying neural mechanism in the human brain remains largely unknown. Here, we study whether a recurrent neural network with Long Short-Term Memory units can mimic an central aspect of human sentence processing, namely the handling of long-distance agreement dependencies. Although the network was solely trained to predict the next word in a large corpus, analysis showed the emergence of a small set of specialized units that successfully handled local and long-distance syntactic agreement for grammatical number. However, simulations showed that this mechanism does not support full recursion and fails when processing some long-range embedded dependencies. We tested those predictions in a behavioral experiment where humans detected violations in number agreement in sentences with systematic variations in the singular/plural status of multiple nouns, with or without embedding. Human and model error patterns were remarkably similar, showing that the model echoes various effects observed in human data. However, a key difference was that, with embedded long-range dependencies, humans remained above chance level, while the model's systematic errors brought it below chance level. Overall, our study shows that exploring the ways in which artificial neural networks process sentences leads to precise and testable hypotheses about human linguistic performance.}

% emergent
% A mechinasm in rnns
% task language model
% NLM -> RNN 